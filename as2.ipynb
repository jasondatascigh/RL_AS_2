{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe4a47f-d444-4d14-adf7-9ec9cfdbab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Grid size\n",
    "GRID_SIZE = 5\n",
    "NUM_STATES = GRID_SIZE * GRID_SIZE\n",
    "\n",
    "# Define special states (row, col)\n",
    "BLUE = (0, 1)\n",
    "GREEN = (0, 3)\n",
    "RED = (4, 1)\n",
    "YELLOW = (4, 3)\n",
    "\n",
    "# Convert 2D state to 1D index\n",
    "def to_index(pos):\n",
    "    return pos[0] * GRID_SIZE + pos[1]\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.95\n",
    "\n",
    "# Actions: up, down, left, right\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up\n",
    "    1: (1, 0),   # Down\n",
    "    2: (0, -1),  # Left\n",
    "    3: (0, 1)    # Right\n",
    "}\n",
    "\n",
    "action_probs = [0.25, 0.25, 0.25, 0.25]  # Equiprobable random policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2596d3-cf9e-41c2-a9f9-b210e291da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    row, col = divmod(state, GRID_SIZE)\n",
    "\n",
    "    # Special states\n",
    "    if (row, col) == BLUE:\n",
    "        return to_index(RED), 5.0\n",
    "\n",
    "    if (row, col) == GREEN:\n",
    "        return (\n",
    "            to_index(RED) if np.random.rand() < 0.5 else to_index(YELLOW),\n",
    "            2.5,\n",
    "        )\n",
    "\n",
    "    # Attempt normal movement\n",
    "    d_row, d_col = actions[action]\n",
    "    new_row, new_col = row + d_row, col + d_col\n",
    "\n",
    "    # Check for boundary\n",
    "    if not (0 <= new_row < GRID_SIZE and 0 <= new_col < GRID_SIZE):\n",
    "        return state, -0.5  # Attempted to step off the grid\n",
    "\n",
    "    return to_index((new_row, new_col)), 0.0  # Normal move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7f509b-8665-449d-807a-bc6185ab01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_bellman_explicit():\n",
    "    A = np.zeros((NUM_STATES, NUM_STATES))\n",
    "    b = np.zeros(NUM_STATES)\n",
    "\n",
    "    for s in range(NUM_STATES):\n",
    "        row, col = divmod(s, GRID_SIZE)\n",
    "\n",
    "        if (row, col) == BLUE:\n",
    "            s_prime = to_index(RED)\n",
    "            A[s, s_prime] = gamma\n",
    "            b[s] = 5.0\n",
    "            continue\n",
    "\n",
    "        if (row, col) == GREEN:\n",
    "            s_red = to_index(RED)\n",
    "            s_yellow = to_index(YELLOW)\n",
    "            A[s, s_red] += 0.5 * gamma\n",
    "            A[s, s_yellow] += 0.5 * gamma\n",
    "            b[s] = 2.5\n",
    "            continue\n",
    "\n",
    "        for a in range(4):\n",
    "            s_prime, r = step(s, a)\n",
    "            A[s, s_prime] += action_probs[a] * gamma\n",
    "            b[s] += action_probs[a] * r\n",
    "\n",
    "    V = np.linalg.solve(np.eye(NUM_STATES) - A, b)\n",
    "    return V.reshape(GRID_SIZE, GRID_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d45cf1-35cb-46bf-9b2d-c97960b8eedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (Explicit Bellman Solution):\n",
      "[[ 1.73  4.07  1.88  1.52  0.16]\n",
      " [ 0.81  1.45  0.96  0.54 -0.11]\n",
      " [-0.04  0.27  0.16 -0.1  -0.53]\n",
      " [-0.7  -0.44 -0.43 -0.61 -0.95]\n",
      " [-1.23 -0.98 -0.94 -1.08 -1.4 ]]\n"
     ]
    }
   ],
   "source": [
    "V_explicit = solve_bellman_explicit()\n",
    "print(\"Value Function (Explicit Bellman Solution):\")\n",
    "print(np.round(V_explicit, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa8e16a-7b5d-4975-8317-7e778d593571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(theta=1e-6, max_iters=1000):\n",
    "    V = np.zeros(NUM_STATES)\n",
    "\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0\n",
    "        V_new = np.zeros(NUM_STATES)\n",
    "\n",
    "        for s in range(NUM_STATES):\n",
    "            row, col = divmod(s, GRID_SIZE)\n",
    "\n",
    "            if (row, col) == BLUE:\n",
    "                V_new[s] = 5.0 + gamma * V[to_index(RED)]\n",
    "                continue\n",
    "\n",
    "            if (row, col) == GREEN:\n",
    "                V_new[s] = 2.5 + gamma * 0.5 * (V[to_index(RED)] + V[to_index(YELLOW)])\n",
    "                continue\n",
    "\n",
    "            value = 0.0\n",
    "            for a in range(4):\n",
    "                s_prime, r = step(s, a)\n",
    "                value += 0.25 * (r + gamma * V[s_prime])\n",
    "            V_new[s] = value\n",
    "\n",
    "            delta = max(delta, abs(V[s] - V_new[s]))\n",
    "\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V.reshape(GRID_SIZE, GRID_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f22d5d-19e2-4d28-a066-fae3abf4f9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (Iterative Evaluation):\n",
      "[[ 1.73  4.07  1.88  1.52  0.16]\n",
      " [ 0.82  1.45  0.96  0.54 -0.11]\n",
      " [-0.04  0.27  0.16 -0.1  -0.53]\n",
      " [-0.7  -0.44 -0.43 -0.61 -0.95]\n",
      " [-1.23 -0.98 -0.94 -1.08 -1.4 ]]\n"
     ]
    }
   ],
   "source": [
    "V_iterative = iterative_policy_evaluation()\n",
    "print(\"Value Function (Iterative Evaluation):\")\n",
    "print(np.round(V_iterative, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a86f35-f272-4671-85dd-4987c0500b9d",
   "metadata": {},
   "source": [
    "The state with the highest value is (0, 1), which is the blue square. This is expected since every action in this state gives a reward of 5 and causes a jump to the red square. States close to it, such as (0, 0), (0, 2), and (0, 3), also have high values because the agent can reach the blue or green square quickly from them. Therefore, the result is not surprising given the reward structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f66d3b-08e2-4271-8ed9-52fc752b2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_policy_and_value():\n",
    "    policy = np.zeros(NUM_STATES, dtype=int)\n",
    "    V = np.zeros(NUM_STATES)\n",
    "    return policy, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80d3414-45ef-42d9-afb0-f46e443d7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, gamma=0.95, theta=1e-3, max_iters=100):\n",
    "    V = np.zeros(NUM_STATES)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        delta = 0\n",
    "        for s in range(NUM_STATES):\n",
    "            row, col = divmod(s, GRID_SIZE)\n",
    "\n",
    "            if (row, col) == BLUE:\n",
    "                V[s] = 5 + gamma * V[to_index(RED)]\n",
    "                continue\n",
    "            elif (row, col) == GREEN:\n",
    "                V[s] = 2.5 + gamma * 0.5 * (V[to_index(RED)] + V[to_index(YELLOW)])\n",
    "                continue\n",
    "\n",
    "            a = policy[s]\n",
    "            s_prime, r = step(s, a)\n",
    "            new_value = r + gamma * V[s_prime]\n",
    "            delta = max(delta, abs(V[s] - new_value))\n",
    "            V[s] = new_value\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e502d2b-2ff4-4e71-95ba-475de320dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, gamma=0.95):\n",
    "    policy_stable = True\n",
    "    new_policy = np.zeros(NUM_STATES, dtype=int)\n",
    "\n",
    "    for s in range(NUM_STATES):\n",
    "        row, col = divmod(s, GRID_SIZE)\n",
    "\n",
    "        if (row, col) in [BLUE, GREEN]:\n",
    "            new_policy[s] = 0\n",
    "            continue\n",
    "\n",
    "        action_values = []\n",
    "        for a in range(4):\n",
    "            s_prime, r = step(s, a)\n",
    "            value = r + gamma * V[s_prime]\n",
    "            action_values.append(value)\n",
    "\n",
    "        best_action = np.argmax(action_values)\n",
    "\n",
    "        if best_action != new_policy[s]:\n",
    "            policy_stable = False\n",
    "\n",
    "        new_policy[s] = best_action\n",
    "\n",
    "    return new_policy, policy_stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9388027-b00a-4686-9e62-2bfc6fd7944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(gamma=0.95, max_outer_iters=200):\n",
    "    policy, V = initialize_policy_and_value()\n",
    "\n",
    "    for i in range(max_outer_iters):\n",
    "        V = policy_evaluation(policy, gamma)\n",
    "        policy, stable = policy_improvement(V, gamma)\n",
    "        if stable:\n",
    "            print(f\"Policy converged after {i+1} iterations.\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"Warning: Reached max iterations without convergence.\")\n",
    "\n",
    "    return policy.reshape(GRID_SIZE, GRID_SIZE), V.reshape(GRID_SIZE, GRID_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ba9cbb5-54eb-460a-a03b-f76f8aa11785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Reached max iterations without convergence.\n",
      "Optimal Policy:\n",
      "[['→' '↑' '←' '↑' '←']\n",
      " ['→' '↑' '↑' '←' '←']\n",
      " ['→' '↑' '↑' '↑' '↑']\n",
      " ['→' '↑' '↑' '↑' '↑']\n",
      " ['→' '↑' '↑' '↑' '↑']]\n",
      "\n",
      "Optimal Value Function:\n",
      "[[20.99 22.1  21.   18.77 17.83]\n",
      " [19.94 21.   19.95 18.95 18.  ]\n",
      " [18.95 19.95 18.95 18.   17.1 ]\n",
      " [18.   18.95 18.   17.1  16.25]\n",
      " [17.1  18.   17.1  16.25 15.43]]\n"
     ]
    }
   ],
   "source": [
    "policy_opt, V_opt = policy_iteration()\n",
    "\n",
    "action_symbols = ['↑', '↓', '←', '→']\n",
    "policy_symbols = np.vectorize(lambda x: action_symbols[x])(policy_opt)\n",
    "\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy_symbols)\n",
    "\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(np.round(V_opt, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d84016b-3d4f-4756-ac1a-dac443d11f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma=0.95, theta=1e-3, max_iters=1000):\n",
    "    V = np.zeros(NUM_STATES)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        delta = 0\n",
    "        V_new = np.zeros_like(V)\n",
    "\n",
    "        for s in range(NUM_STATES):\n",
    "            row, col = divmod(s, GRID_SIZE)\n",
    "\n",
    "            if (row, col) == BLUE:\n",
    "                V_new[s] = 5 + gamma * V[to_index(RED)]\n",
    "                continue\n",
    "            elif (row, col) == GREEN:\n",
    "                V_new[s] = 2.5 + gamma * 0.5 * (V[to_index(RED)] + V[to_index(YELLOW)])\n",
    "                continue\n",
    "\n",
    "            max_value = float('-inf')\n",
    "            for a in range(4):\n",
    "                s_prime, r = step(s, a)\n",
    "                val = r + gamma * V[s_prime]\n",
    "                max_value = max(max_value, val)\n",
    "\n",
    "            V_new[s] = max_value\n",
    "            delta = max(delta, abs(V[s] - V_new[s]))\n",
    "\n",
    "        V = V_new\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b75f9d-5b1c-4e1c-b40b-34d8117293c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy_from_value(V, gamma=0.95):\n",
    "    policy = np.zeros(NUM_STATES, dtype=int)\n",
    "\n",
    "    for s in range(NUM_STATES):\n",
    "        row, col = divmod(s, GRID_SIZE)\n",
    "\n",
    "        if (row, col) in [BLUE, GREEN]:\n",
    "            policy[s] = 0\n",
    "            continue\n",
    "\n",
    "        best_action = 0\n",
    "        best_value = float('-inf')\n",
    "        for a in range(4):\n",
    "            s_prime, r = step(s, a)\n",
    "            val = r + gamma * V[s_prime]\n",
    "            if val > best_value:\n",
    "                best_value = val\n",
    "                best_action = a\n",
    "\n",
    "        policy[s] = best_action\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f57ff041-4fc4-4f10-841b-fe3d6a4d9188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 0\n",
      "Optimal Value Function from Value Iteration:\n",
      "[[0.  5.  0.  2.5 0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]]\n",
      "\n",
      "Optimal Policy (Value Iteration):\n",
      "[['→' '↑' '←' '↑' '←']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' '↑']]\n"
     ]
    }
   ],
   "source": [
    "V_vi = value_iteration()\n",
    "policy_vi = extract_policy_from_value(V_vi)\n",
    "\n",
    "V_vi = V_vi.reshape(GRID_SIZE, GRID_SIZE)\n",
    "policy_vi = policy_vi.reshape(GRID_SIZE, GRID_SIZE)\n",
    "\n",
    "print(\"Optimal Value Function from Value Iteration:\")\n",
    "print(np.round(V_vi, 2))\n",
    "\n",
    "action_symbols = ['↑', '↓', '←', '→']\n",
    "policy_symbols = np.vectorize(lambda x: action_symbols[x])(policy_vi)\n",
    "\n",
    "print(\"\\nOptimal Policy (Value Iteration):\")\n",
    "print(policy_symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64863a57-dd1d-46be-a363-0cbdbfaf77d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dc6649e-9bec-40cc-850d-89ab4d8ef83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminal states: Black squares\n",
    "TERMINAL_STATES = [to_index((2, 2)), to_index((4, 4))]\n",
    "\n",
    "def is_terminal(state):\n",
    "    return state in TERMINAL_STATES\n",
    "\n",
    "# New step function for Part 2\n",
    "def step2(state, action):\n",
    "    if is_terminal(state):\n",
    "        return state, 0, True  # Stays in terminal, episode ends\n",
    "\n",
    "    row, col = divmod(state, GRID_SIZE)\n",
    "\n",
    "    if (row, col) == BLUE:\n",
    "        return to_index(RED), 5.0, False\n",
    "\n",
    "    if (row, col) == GREEN:\n",
    "        target = RED if np.random.rand() < 0.5 else YELLOW\n",
    "        return to_index(target), 2.5, False\n",
    "\n",
    "    d_row, d_col = actions[action]\n",
    "    new_row, new_col = row + d_row, col + d_col\n",
    "\n",
    "    # Off-grid move\n",
    "    if not (0 <= new_row < GRID_SIZE and 0 <= new_col < GRID_SIZE):\n",
    "        return state, -0.5, False\n",
    "\n",
    "    new_state = to_index((new_row, new_col))\n",
    "    done = is_terminal(new_state)\n",
    "    return new_state, -0.2, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9bbec61-0978-4e79-a2d1-3dd209c6994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_episode_es(policy, gamma=0.95):\n",
    "    states = list(range(NUM_STATES))\n",
    "    start_state = random.choice([s for s in states if not is_terminal(s)])\n",
    "    start_action = random.choice(list(actions.keys()))\n",
    "    \n",
    "    episode = []\n",
    "    state = start_state\n",
    "    action = start_action\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        next_state, reward, done = step2(state, action)\n",
    "        episode.append((state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "        action = random.choices(list(actions.keys()), weights=policy[state])[0]\n",
    "        state = next_state\n",
    "\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "386222fb-d721-4b6f-8dde-4ddbc7167b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_exploring_starts(num_episodes=1000, gamma=0.95):\n",
    "    Q = defaultdict(lambda: np.zeros(4))\n",
    "    returns = defaultdict(list)\n",
    "\n",
    "    policy = {s: [0.25] * 4 for s in range(NUM_STATES)}\n",
    "\n",
    "    for episode_idx in range(num_episodes):\n",
    "        episode = generate_episode_es(policy, gamma)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, r = episode[t]\n",
    "            G = gamma * G + r\n",
    "\n",
    "            if (s, a) not in visited:\n",
    "                returns[(s, a)].append(G)\n",
    "                Q[s][a] = np.mean(returns[(s, a)])\n",
    "                visited.add((s, a))\n",
    "\n",
    "        for s in policy:\n",
    "            if is_terminal(s):\n",
    "                continue\n",
    "            best_a = np.argmax(Q[s])\n",
    "            policy[s] = [1 if a == best_a else 0 for a in range(4)]\n",
    "\n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b746a8a-0236-45ee-883c-bd8492156010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece2939-520b-4007-9652-7cf6fe4c0766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1083f7f-614a-4a6c-8085-7130bdfb95b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
